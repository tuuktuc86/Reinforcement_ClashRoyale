{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a5222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pyautogui as pag\n",
    "import PIL\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import time\n",
    "import seaborn as sns\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "pag.FAILSAFE = False\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import torch.multiprocessing as _mp\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1729b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.py\n",
    "class ENV():\n",
    "    def __init__(self):\n",
    "        # screenshot의 위치 지정, 클래스 생성할때 가져오기\n",
    "\n",
    "        # winflg와 lose flag 존재해야 함. 0으로 하는 건 grayscale\n",
    "        self.winFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/weWin.png', 0)\n",
    "        self.loseFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemyWin.png', 0)\n",
    "        # nocard flag\n",
    "        self.nocardFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/nocard.png')\n",
    "        # noElixir flag\n",
    "        self.noelixirFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/noElixir.png')\n",
    "        self.startGameFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/battleStart.png')\n",
    "        self.finishFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/battleFinish.png')\n",
    "        self.enemy1Flag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemy1.png')\n",
    "        self.enemy2Flag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemy2.png')\n",
    "        self.enemy3Flag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemy3.png')\n",
    "    def return_state(self, img):\n",
    "        # 스크린 샷을 인자로 받아와서 모델에 넣을 수 있도록 tensor로 변환\n",
    "        tf = transforms.ToTensor()\n",
    "        img_t = tf(img) # time.sleep(0.3)\n",
    "        img_t = img_t.unsqueeze(0)\n",
    "        # img_t = img_t.permute(1, 0, 2, 3)\n",
    "\n",
    "        return img_t\n",
    "\n",
    "    def check_finish(self,img):\n",
    "        finishMessage = cv2.cvtColor(self.finishFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(finishMessage, img, cv2.TM_CCOEFF_NORMED)\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def check_win(self, img):\n",
    "        # 게임이 이겼는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        # img = np.array(img)\n",
    "        checkFlag1 = np.array(img.crop((225, 335, 280, 365)))\n",
    "        checkFlag1 = cv2.cvtColor(checkFlag1, cv2.COLOR_BGR2GRAY)\n",
    "        win_check = cv2.matchTemplate(checkFlag1, self.winFlag, cv2.TM_CCOEFF_NORMED)\n",
    "        if win_check > 0.8:\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "           \n",
    "    def check_lose(self, img):\n",
    "        # 게임이 졌는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        # img = np.array(img)\n",
    "        checkFlag2 = np.array(img.crop((225, 85, 280, 115)))\n",
    "        checkFlag2 = cv2.cvtColor(checkFlag2, cv2.COLOR_BGR2GRAY)\n",
    "        lose_check = cv2.matchTemplate(checkFlag2, self.loseFlag, cv2.TM_CCOEFF_NORMED)\n",
    "        if lose_check > 0.8:\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def check_card(self, img):\n",
    "        # 카드를 선택하지 않았는지 확인, screenshot을 가져와서 init에 지정된 nocard 이미지와 비교하여 reward 부여\n",
    "        nocard = cv2.cvtColor(self.nocardFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(nocard, img, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            # print(np.max(ratio))\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def check_elixir(self, img):\n",
    "        noElixir = cv2.cvtColor(self.noelixirFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(noElixir, img, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            # print(np.max(ratio))\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def checkET1(self, img):\n",
    "        score1 = 0\n",
    "        checkFlag1 = np.array(img.crop((105, 135, 155, 136)))\n",
    "\n",
    "        for i in range(50):\n",
    "            if (checkFlag1[0][i][0] >= 96):\n",
    "                score1 += 1\n",
    "\n",
    "        score1 = score1 * 2\n",
    "        return score1\n",
    "\n",
    "    def checkET2(self, img):\n",
    "        score2 = 0\n",
    "        checkFlag2 = np.array(img.crop((371, 135, 421, 136)))\n",
    "\n",
    "        for i in range(50):\n",
    "            if (checkFlag2[0][i][0] >= 96):\n",
    "                score2 += 1\n",
    "\n",
    "        score2 = score2 * 2\n",
    "        return score2\n",
    "\n",
    "    def checkGameStart(self, img):\n",
    "        startMessage = cv2.cvtColor(self.startGameFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(startMessage, img, cv2.TM_CCOEFF_NORMED)\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            # print(np.max(ratio))\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def enemy1(self, img):\n",
    "        crownFlag1 = cv2.cvtColor(self.enemy1Flag, cv2.COLOR_BGR2GRAY)\n",
    "        img1 = np.array(img)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(crownFlag1, img1, cv2.TM_CCOEFF_NORMED)\n",
    "        #print(np.max(ratio))\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def enemy2(self, img):\n",
    "        crownFlag2 = cv2.cvtColor(self.enemy2Flag, cv2.COLOR_BGR2GRAY)\n",
    "        img1 = np.array(img)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(crownFlag2, img1, cv2.TM_CCOEFF_NORMED)\n",
    "        #print(np.max(ratio))\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def enemy3(self, img):\n",
    "        crownFlag3 = cv2.cvtColor(self.enemy3Flag, cv2.COLOR_BGR2GRAY)\n",
    "        img1 = np.array(img)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(crownFlag3, img1, cv2.TM_CCOEFF_NORMED)\n",
    "        #print(np.max(ratio))\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # def checkOT1\n",
    "    # def checkOT1\n",
    "    # def checkOT1\n",
    "\n",
    "    # 우리 타워와 상대 타워의 hp를 확인하여 reward 부여\n",
    "\n",
    "    def retryGame(self):\n",
    "        time.sleep(8)\n",
    "        pag.click((2860, 875))\n",
    "        time.sleep(5)\n",
    "        pag.click((3070, 185))\n",
    "        time.sleep(5)\n",
    "        pag.click((2920, 385))\n",
    "        time.sleep(3)\n",
    "        pag.click((2950, 615))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21263c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "gamma = 0.98\n",
    "lmbda = 0.95\n",
    "eps_clip = 0.1\n",
    "K_epoch = 10\n",
    "T_horizon = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(PPO, self).__init__()\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "                                   # nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.actor_linear = nn.Sequential(nn.Linear(64, 64),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(64, num_actions))\n",
    "        self.critic_linear = nn.Sequential(nn.Linear(64, 64),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(64, 1))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "    \n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # x = self.actor_linear(x.reshape(x.size(0), -1))\n",
    "        x = self.avgpool(x).reshape(x.size(0), -1)\n",
    "        x = self.actor_linear(x)\n",
    "        prob = F.softmax(x, dim = softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.avgpool(x).reshape(x.size(0), -1)\n",
    "        v = self.critic_linear(x.reshape(x.size(0), -1))\n",
    "        return v\n",
    "    \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        \n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "             \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append(prob_a)\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s_lst = torch.stack(s_lst).cuda()\n",
    "        s_prime_lst = torch.stack(s_prime_lst).cuda()\n",
    "        #print(f\"type_slst = {type(s_lst)}\")\n",
    "        s,a,r,s_prime,done_mask, prob_a = s_lst, torch.tensor(a_lst).cuda(), \\\n",
    "                                          torch.tensor(r_lst).cuda(), s_prime_lst, \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float).cuda(), torch.tensor(prob_a_lst).cuda()\n",
    "        #print(s.shape)\n",
    "        s = s.squeeze()\n",
    "        s_prime = s_prime.squeeze()\n",
    "        s, s_prime, = s.cuda(), s_prime.cuda() \n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "            \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "        #print(f\"s size = {len(s)}, s_prime_size - {len(s_prime)}\")\n",
    "        \n",
    "        global train_time\n",
    "        #print(f\"a = {a}\")\n",
    "        for i in range(K_epoch):\n",
    "            train_time += 1\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.to(\"cpu\")\n",
    "            delta = delta.detach().numpy()           \n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype = torch.float).cuda()\n",
    "            \n",
    "            pi = self.pi(s, softmax_dim = 1)\n",
    "            pi_a = pi.gather(1, a)\n",
    "            #print(f\"pi_a = {pi_a}, prob_A = {prob_a}\")\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            #print(f\"advantage = {advantage}\")\n",
    "            #print(\"=================\")\n",
    "            #print(f\"surr1 = {surr1}, surr2 = {surr2}\")\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s), td_target.detach())\n",
    "            #print(f\"loss1 = {loss.mean()}\")\n",
    "            writer.add_scalar(\"Loss/train\", loss.mean(), train_time)\n",
    "            writer.flush()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            #loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(f\"{i+1} train is over\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833519ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py envs = MultipleEnvironments(opt.world, opt.stage, opt.action_type, opt.num_processes)\n",
    "env = ENV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025b464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py model = PPO(envs.num_states, envs.num_actions)\n",
    "model = PPO(45).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a24b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = []\n",
    "action_list_name = {}\n",
    "count = 0\n",
    "for i in range(5):\n",
    "    for j in range(8):\n",
    "        action_list.append((2655+j*58.571, 510+i*63.75))\n",
    "        \n",
    "        action_list_name[count] = str(i) + ' ' + str(j)\n",
    "        count+=1\n",
    "        \n",
    "action_list.append([2765, 900])\n",
    "action_list.append([2855, 940])\n",
    "action_list.append([2950, 900])\n",
    "action_list.append([3055, 900])\n",
    "action_list.append([-1, -1])\n",
    "action_list_name[40] = \"card 1\"\n",
    "action_list_name[41] = \"card 2\"\n",
    "action_list_name[42] = \"card 3\"\n",
    "action_list_name[43] = \"card 4\"\n",
    "action_list_name[44] = \"rest action\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8049881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2655.0, 510.0), (2713.571, 510.0), (2772.142, 510.0), (2830.713, 510.0), (2889.284, 510.0), (2947.855, 510.0), (3006.426, 510.0), (3064.997, 510.0), (2655.0, 573.75), (2713.571, 573.75), (2772.142, 573.75), (2830.713, 573.75), (2889.284, 573.75), (2947.855, 573.75), (3006.426, 573.75), (3064.997, 573.75), (2655.0, 637.5), (2713.571, 637.5), (2772.142, 637.5), (2830.713, 637.5), (2889.284, 637.5), (2947.855, 637.5), (3006.426, 637.5), (3064.997, 637.5), (2655.0, 701.25), (2713.571, 701.25), (2772.142, 701.25), (2830.713, 701.25), (2889.284, 701.25), (2947.855, 701.25), (3006.426, 701.25), (3064.997, 701.25), (2655.0, 765.0), (2713.571, 765.0), (2772.142, 765.0), (2830.713, 765.0), (2889.284, 765.0), (2947.855, 765.0), (3006.426, 765.0), (3064.997, 765.0), [2765, 900], [2855, 940], [2950, 900], [3055, 900], [-1, -1]]\n",
      "{0: '0 0', 1: '0 1', 2: '0 2', 3: '0 3', 4: '0 4', 5: '0 5', 6: '0 6', 7: '0 7', 8: '1 0', 9: '1 1', 10: '1 2', 11: '1 3', 12: '1 4', 13: '1 5', 14: '1 6', 15: '1 7', 16: '2 0', 17: '2 1', 18: '2 2', 19: '2 3', 20: '2 4', 21: '2 5', 22: '2 6', 23: '2 7', 24: '3 0', 25: '3 1', 26: '3 2', 27: '3 3', 28: '3 4', 29: '3 5', 30: '3 6', 31: '3 7', 32: '4 0', 33: '4 1', 34: '4 2', 35: '4 3', 36: '4 4', 37: '4 5', 38: '4 6', 39: '4 7', 40: 'card 1', 41: 'card 2', 42: 'card 3', 43: 'card 4', 44: 'rest action'}\n"
     ]
    }
   ],
   "source": [
    "print(action_list)\n",
    "print(action_list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a012accc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 13:50:49.519171: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9087a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py curr_episode = 0\n",
    "curr_episode = 0\n",
    "train_time = 0\n",
    "play_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582f004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    curr_episode += 1\n",
    "    \n",
    "    \n",
    "    #reward 초기화\n",
    "    r = 0\n",
    "    \n",
    "    #done 초기화\n",
    "    done = 0\n",
    "    \n",
    "    # enemy tower score / score1 = left, score2 = right\n",
    "    score1 = 100\n",
    "    score2 = 100\n",
    "    \n",
    "    num_local_steps = 0\n",
    "    \n",
    "    #set start_flag\n",
    "    start_flag = 0\n",
    "    \n",
    "    #승리 패배를 기록\n",
    "    win_lose_flag = 0\n",
    "    \n",
    "    #s라는 변수는 이렇다는 것을 보여주기 위한 구문\n",
    "    img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "    s = env.return_state(img).cuda()\n",
    "    right_click_flag = 0\n",
    "    play_time += 1\n",
    "    while(1):\n",
    "    #for _ in range(40):\n",
    "        #opt 기본 num_local_steps = 512\n",
    "        \n",
    "        #게임 시작 확인 전까지 멈춤 지시\n",
    "        while (start_flag == 0):\n",
    "            # 스크린샷 찍기\n",
    "            img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "\n",
    "            if (env.checkGameStart(img)):\n",
    "                print(\"game start\")\n",
    "                start_flag = 1\n",
    "                time.sleep(3)\n",
    "                #현재 이미지 캡처 후 변환\n",
    "                img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "                s = env.return_state(img).cuda()\n",
    "                break\n",
    "                \n",
    "#         img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "#         s = env.return_state(img).cuda()\n",
    "        for t in range(T_horizon):\n",
    "            \n",
    "        \n",
    "            #num_local_step_count\n",
    "            num_local_steps += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            prob = model.pi(s, softmax_dim=1)\n",
    "            #print(f\"prob = {prob}\")\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample().item()\n",
    "            if(a >= 40 and a <= 43):\n",
    "                right_click_flag = 1\n",
    "                \n",
    "            if(right_click_flag == 1 and a >= 0 and a <= 39):\n",
    "                print(\"right_click\")\n",
    "                r+= 50\n",
    "                right_click_flag = 0\n",
    "                \n",
    "            #print(f\"action = {action_list_name[a]}\")\n",
    "            print(f\"a = {a}\")\n",
    "            #pag 클릭. rest action은 -1 -1 이므로 실행하지 않음\n",
    "            if(a!=44):\n",
    "                # 화면 클릭\n",
    "                pag.click(action_list[a][0], action_list[a][1])\n",
    "                pass\n",
    "            \n",
    "            #클릭 한번당 reward -1\n",
    "            r -= 1\n",
    "            \n",
    "            # 승리 확인\n",
    "            if (env.check_win(img)):\n",
    "                print(\"win\")\n",
    "                r += 5000\n",
    "                done = 1\n",
    "                win_lose_flag = 1\n",
    "                \n",
    "            # 패배 확인\n",
    "            if (env.check_lose(img)):\n",
    "                print(\"lose\")\n",
    "                r -= 5000\n",
    "                done = 1\n",
    "                win_lose_flag = -1\n",
    "                \n",
    "            # no card확인\n",
    "            if (env.check_card(img)):\n",
    "                r -= 20\n",
    "                print(\"no card\")\n",
    "\n",
    "            # no elixir확인\n",
    "            if (env.check_elixir(img)):\n",
    "                r -= 20\n",
    "                print(\"no Elixir\")\n",
    "\n",
    "            if (env.enemy1(img)):\n",
    "                r -= 100\n",
    "                print(\"crown - 1\")\n",
    "\n",
    "            if (env.enemy2(img)):\n",
    "                r -= 100\n",
    "                print(\"crown - 2\")\n",
    "\n",
    "            if (env.enemy3(img)):\n",
    "                r -= 5000\n",
    "                done = 1\n",
    "                print(\"crown - 3\")\n",
    "\n",
    "\n",
    "            # enemy tower reward calculate\n",
    "            score1_now = env.checkET1(img)\n",
    "            score2_now = env.checkET2(img)\n",
    "            #print(f\"score1 = {score1}, score1_now = {score1_now}\")\n",
    "            #print(f\"score2 = {score2}, score2_now = {score2_now}\")\n",
    "            print(f\"left tower HP = {score1}, right tower HP = {score2}\")\n",
    "            print(f\"reward = {r}\")\n",
    "            if (score1_now < score1):\n",
    "                r += 10 * (score1 - score1_now)\n",
    "                score1 = score1_now\n",
    "\n",
    "            if (score2_now < score2):\n",
    "                r += 105 * (score2 - score2_now)\n",
    "                score2 = score2_now\n",
    "                \n",
    "            \n",
    "            #현재 이미지 캡처 후 변환\n",
    "            img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "            s_prime = env.return_state(img).cuda()\n",
    "            #print(prob)\n",
    "            model.put_data((s, a, r, s_prime, prob[0][a], done))\n",
    "            \n",
    "            s = s_prime\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "#         print(\"train before\")\n",
    "#         print(model.conv1[0].weight)\n",
    "#         print(\"=====================\")\n",
    "        model.train_net()\n",
    "#         print(\"train after\")\n",
    "#         print(model.conv1[0].weight)\n",
    "        #print(f\"loss = {loss}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"how many time to train? = {train_time}\")\n",
    "            #opt.save_interval = 50\n",
    "        \n",
    "        save_interval = 500\n",
    "        if(train_time % save_interval == 0):\n",
    "            #print(\"Episode: {}. Total loss: {}, save_interval = {}\".format(curr_episode, total_loss, curr_episode % save_interval))\n",
    "            torch.save(model.state_dict(),\"{}/clash_royale_PPO_v2_{}\".format(\"trained_models\", train_time))\n",
    "        \n",
    "        if done:\n",
    "            writer.add_scalar(\"reward/epi\", r, play_time)\n",
    "            writer.flush()\n",
    "            writer.add_scalar(\"clicks/epi\", num_local_steps, play_time)\n",
    "            writer.flush()\n",
    "            writer.add_scalar(\"win_lose/epi\", win_lose_flag, play_time)\n",
    "            writer.flush()\n",
    "            break\n",
    "    print(\"retry game\")\n",
    "    env.retryGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d05d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "pag.click((2855, 940))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef150f16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mpag\u001b[49m\u001b[38;5;241m.\u001b[39mscreenshot(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemyWin.png\u001b[39m\u001b[38;5;124m'\u001b[39m,region\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2830\u001b[39m, \u001b[38;5;241m185\u001b[39m, \u001b[38;5;241m2885\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2830\u001b[39m, \u001b[38;5;241m30\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#clickEmpty1 = clickEmpty1.crop((2737, 573, 2987, 600))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(type(clickEmpty1))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pag' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6037b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
