{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4a5222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pyautogui as pag\n",
    "import PIL\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import time\n",
    "import seaborn as sns\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "pag.FAILSAFE = False\n",
    "\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import torch.multiprocessing as _mp\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1729b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.py\n",
    "class ENV():\n",
    "    def __init__(self):\n",
    "        # screenshot의 위치 지정, 클래스 생성할때 가져오기\n",
    "\n",
    "        # winflg와 lose flag 존재해야 함. 0으로 하는 건 grayscale\n",
    "        self.winFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/weWin.png', 0)\n",
    "        self.loseFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemyWin.png', 0)\n",
    "        # nocard flag\n",
    "        self.nocardFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/nocard.png')\n",
    "        # noElixir flag\n",
    "        self.noelixirFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/noElixir.png')\n",
    "        self.startGameFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/battleStart.png')\n",
    "        self.finishFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/battleFinish.png')\n",
    "        self.enemy1Flag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemy1.png')\n",
    "        self.enemy2Flag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemy2.png')\n",
    "        self.enemy3Flag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemy3.png')\n",
    "    def return_state(self, img):\n",
    "        # 스크린 샷을 인자로 받아와서 모델에 넣을 수 있도록 tensor로 변환\n",
    "        tf = transforms.ToTensor()\n",
    "        img_t = tf(img) # time.sleep(0.3)\n",
    "        img_t = img_t.unsqueeze(0)\n",
    "        # img_t = img_t.permute(1, 0, 2, 3)\n",
    "\n",
    "        return img_t\n",
    "\n",
    "    def check_finish(self,img):\n",
    "        finishMessage = cv2.cvtColor(self.finishFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(finishMessage, img, cv2.TM_CCOEFF_NORMED)\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def check_win(self, img):\n",
    "        # 게임이 이겼는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        # img = np.array(img)\n",
    "        checkFlag1 = np.array(img.crop((225, 335, 280, 365)))\n",
    "        checkFlag1 = cv2.cvtColor(checkFlag1, cv2.COLOR_BGR2GRAY)\n",
    "        win_check = cv2.matchTemplate(checkFlag1, self.winFlag, cv2.TM_CCOEFF_NORMED)\n",
    "        if win_check > 0.8:\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "           \n",
    "    def check_lose(self, img):\n",
    "        # 게임이 졌는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        # img = np.array(img)\n",
    "        checkFlag2 = np.array(img.crop((225, 85, 280, 115)))\n",
    "        checkFlag2 = cv2.cvtColor(checkFlag2, cv2.COLOR_BGR2GRAY)\n",
    "        lose_check = cv2.matchTemplate(checkFlag2, self.loseFlag, cv2.TM_CCOEFF_NORMED)\n",
    "        if lose_check > 0.8:\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def check_card(self, img):\n",
    "        # 카드를 선택하지 않았는지 확인, screenshot을 가져와서 init에 지정된 nocard 이미지와 비교하여 reward 부여\n",
    "        nocard = cv2.cvtColor(self.nocardFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(nocard, img, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            # print(np.max(ratio))\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def check_elixir(self, img):\n",
    "        noElixir = cv2.cvtColor(self.noelixirFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(noElixir, img, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            # print(np.max(ratio))\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def checkET1(self, img):\n",
    "        score1 = 0\n",
    "        checkFlag1 = np.array(img.crop((105, 138, 155, 139)))\n",
    "\n",
    "        for i in range(50):\n",
    "            if (checkFlag1[0][i][0] >= 96):\n",
    "                score1 += 1\n",
    "\n",
    "        score1 = score1 * 2\n",
    "        return score1\n",
    "\n",
    "    def checkET2(self, img):\n",
    "        score2 = 0\n",
    "        checkFlag2 = np.array(img.crop((371, 138, 421, 139)))\n",
    "\n",
    "        for i in range(50):\n",
    "            if (checkFlag2[0][i][0] >= 96):\n",
    "                score2 += 1\n",
    "\n",
    "        score2 = score2 * 2\n",
    "        return score2\n",
    "\n",
    "    def checkGameStart(self, img):\n",
    "        startMessage = cv2.cvtColor(self.startGameFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(startMessage, img, cv2.TM_CCOEFF_NORMED)\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            # print(np.max(ratio))\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def enemy1(self, img):\n",
    "        crownFlag1 = cv2.cvtColor(self.enemy1Flag, cv2.COLOR_BGR2GRAY)\n",
    "        img1 = np.array(img)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(crownFlag1, img1, cv2.TM_CCOEFF_NORMED)\n",
    "        #print(np.max(ratio))\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def enemy2(self, img):\n",
    "        crownFlag2 = cv2.cvtColor(self.enemy2Flag, cv2.COLOR_BGR2GRAY)\n",
    "        img1 = np.array(img)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(crownFlag2, img1, cv2.TM_CCOEFF_NORMED)\n",
    "        #print(np.max(ratio))\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def enemy3(self, img):\n",
    "        crownFlag3 = cv2.cvtColor(self.enemy3Flag, cv2.COLOR_BGR2GRAY)\n",
    "        img1 = np.array(img)\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(crownFlag3, img1, cv2.TM_CCOEFF_NORMED)\n",
    "        #print(np.max(ratio))\n",
    "        if (np.max(ratio) > 0.90):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # def checkOT1\n",
    "    # def checkOT1\n",
    "    # def checkOT1\n",
    "\n",
    "    # 우리 타워와 상대 타워의 hp를 확인하여 reward 부여\n",
    "\n",
    "    def retryGame(self):\n",
    "        time.sleep(3)\n",
    "        pag.click((2860, 875))\n",
    "        time.sleep(5)\n",
    "        pag.click((3070, 185))\n",
    "        time.sleep(5)\n",
    "        pag.click((2920, 385))\n",
    "        time.sleep(3)\n",
    "        pag.click((2950, 615))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fdbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(PPO, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.actor_linear = nn.Sequential(nn.Linear(462848, 256),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(256, num_actions))\n",
    "        self.critic_linear = nn.Sequential(nn.Linear(462848, 256),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(256, 1))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #x = F.softmax(x, dim=softmax_dim)\n",
    "        return self.actor_linear(x), self.critic_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "833519ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py envs = MultipleEnvironments(opt.world, opt.stage, opt.action_type, opt.num_processes)\n",
    "env = ENV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "025b464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py model = PPO(envs.num_states, envs.num_actions)\n",
    "model = PPO(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48e10afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = [[2720, 550], [2860, 550], [3000, 550], [2720 ,660], [2860, 660], [3000, 660], [2765, 900], [2855, 930], [2950, 900], [3055, 900], [-1, -1]]\n",
    "action_list_name = {0:'left top', 1:'center top', 2:'right top', 3:'right bottom', 4:'center bottom', 5:'center right', 6:'card 1', 7:'card 2', 8:'card 3', 9:'card 4', 10:'rest action'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56a64491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a012accc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9087a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py curr_episode = 0\n",
    "curr_episode = 0\n",
    "train_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9582f004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([9])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([5])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([4])\n",
      "no card\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([7])\n",
      "no card\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([0])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([1])\n",
      "no card\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([7])\n",
      "no card\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([6])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([1])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([7])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([10])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([3])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([10])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([4])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([0])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([7])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([10])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([1])\n",
      "left tower HP = 100, right tower HP = 100\n",
      "tensor([1])\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([9])\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([10])\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([1])\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([0])\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([2])\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([8])\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([3])\n",
      "no Elixir\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([4])\n",
      "no Elixir\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([9])\n",
      "no card\n",
      "left tower HP = 98, right tower HP = 100\n",
      "tensor([0])\n",
      "left tower HP = 84, right tower HP = 100\n",
      "tensor([5])\n",
      "no Elixir\n",
      "left tower HP = 74, right tower HP = 100\n",
      "tensor([4])\n",
      "no Elixir\n",
      "left tower HP = 68, right tower HP = 100\n",
      "tensor([0])\n",
      "no card\n",
      "left tower HP = 60, right tower HP = 100\n",
      "tensor([6])\n",
      "no card\n",
      "left tower HP = 60, right tower HP = 100\n",
      "tensor([9])\n",
      "crown - 1\n",
      "left tower HP = 58, right tower HP = 98\n",
      "tensor([10])\n",
      "crown - 1\n",
      "left tower HP = 58, right tower HP = 98\n",
      "tensor([8])\n",
      "left tower HP = 58, right tower HP = 98\n",
      "tensor([0])\n",
      "no Elixir\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([5])\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([8])\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([2])\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([7])\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([4])\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([3])\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([4])\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([9])\n",
      "no Elixir\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([1])\n",
      "no Elixir\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([1])\n",
      "crown - 3\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([0])\n",
      "no card\n",
      "crown - 3\n",
      "left tower HP = 56, right tower HP = 98\n",
      "tensor([10])\n",
      "left tower HP = 38, right tower HP = 80\n",
      "tensor([9])\n",
      "left tower HP = 38, right tower HP = 80\n",
      "tensor([10])\n",
      "left tower HP = 0, right tower HP = 0\n",
      "tensor([3])\n",
      "lose\n",
      "left tower HP = 0, right tower HP = 0\n",
      "tensor([4])\n",
      "lose\n",
      "how many time to train? = 1\n",
      "how many time to train? = 2\n",
      "Episode: 3. Total loss: 13590.666015625, save_interval = 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 265\u001b[0m\n\u001b[1;32m    263\u001b[0m     save_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Total loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, save_interval = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(curr_episode, total_loss, curr_episode \u001b[38;5;241m%\u001b[39m save_interval))\n\u001b[0;32m--> 265\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretryGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 165\u001b[0m, in \u001b[0;36mENV.retryGame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    164\u001b[0m pag\u001b[38;5;241m.\u001b[39mclick((\u001b[38;5;241m2860\u001b[39m, \u001b[38;5;241m875\u001b[39m))\n\u001b[0;32m--> 165\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m pag\u001b[38;5;241m.\u001b[39mclick((\u001b[38;5;241m3070\u001b[39m, \u001b[38;5;241m185\u001b[39m))\n\u001b[1;32m    167\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    curr_episode += 1\n",
    "    old_log_policies = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    states = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "    #reward 초기화\n",
    "    reward = 0\n",
    "    \n",
    "    #done 초기화\n",
    "    done = 0\n",
    "    \n",
    "    # enemy tower score / score1 = left, score2 = right\n",
    "    score1 = 100\n",
    "    score2 = 100\n",
    "    \n",
    "    #img to curr states\n",
    "    img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "    curr_states = env.return_state(img)\n",
    "    \n",
    "    num_local_steps = 0\n",
    "    \n",
    "    #set start_flag\n",
    "    start_flag = 0\n",
    "    \n",
    "    while(1):\n",
    "    #for _ in range(40):\n",
    "        #opt 기본 num_local_steps = 512\n",
    "        \n",
    "        #게임 시작 확인 전까지 멈춤 지시\n",
    "        while (start_flag == 0):\n",
    "            # 스크린샷 찍기\n",
    "            img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "\n",
    "            if (env.checkGameStart(img)):\n",
    "                # print(\"game start\")\n",
    "                start_flag = 1\n",
    "                time.sleep(3)\n",
    "                break\n",
    "        \n",
    "        \n",
    "        #num_local_step_count\n",
    "        num_local_steps += 1\n",
    "        \n",
    "        #현재 이미지 캡처 후 변환\n",
    "        img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "        curr_states = env.return_state(img)\n",
    "        \n",
    "        # 승리 확인\n",
    "        if (env.check_win(img)):\n",
    "            print(\"win\")\n",
    "            reward += 5000\n",
    "            done = 1\n",
    "        # 패배 확인\n",
    "        if (env.check_lose(img)):\n",
    "            print(\"lose\")\n",
    "            reward -= 5000\n",
    "            done = 1\n",
    "        # no card확인\n",
    "        if (env.check_card(img)):\n",
    "            reward -= 1\n",
    "            print(\"no card\")\n",
    "\n",
    "        # no elixir확인\n",
    "        if (env.check_elixir(img)):\n",
    "            reward -= 1\n",
    "            print(\"no Elixir\")\n",
    "            \n",
    "        if (env.enemy1(img)):\n",
    "            reward -= 100\n",
    "            print(\"crown - 1\")\n",
    "        \n",
    "        if (env.enemy2(img)):\n",
    "            reward -= 100\n",
    "            print(\"crown - 2\")\n",
    "        \n",
    "        if (env.enemy3(img)):\n",
    "            reward -= 5000\n",
    "            done = 1\n",
    "            print(\"crown - 3\")\n",
    "\n",
    "\n",
    "        # enemy tower reward calculate\n",
    "        score1_now = env.checkET1(img)\n",
    "        score2_now = env.checkET2(img)\n",
    "        #print(f\"score1 = {score1}, score1_now = {score1_now}\")\n",
    "        #print(f\"score2 = {score2}, score2_now = {score2_now}\")\n",
    "        print(f\"left tower HP = {score1}, right tower HP = {score2}\")\n",
    "        if (score1_now < score1):\n",
    "            reward += 5 * (score1 - score1_now)\n",
    "            score1 = score1_now\n",
    "\n",
    "        if (score2_now < score2):\n",
    "            reward += 5 * (score2 - score2_now)\n",
    "            score2 = score2_now\n",
    "        \n",
    "        \n",
    "        \n",
    "        #train.py states.append(curr_states)\n",
    "        states.append(curr_states)\n",
    "        \n",
    "        #logits, value = model(curr_states)\n",
    "        logits, value = model(curr_states)\n",
    "        \n",
    "        #values.append(value.squeeze())\n",
    "        values.append(value.squeeze())\n",
    "        \n",
    "        #policy = F.softmax(logits, dim=1)\n",
    "        policy = F.softmax(logits, dim=1)\n",
    "        \n",
    "        #old_m = Categorical(policy)\n",
    "        old_m = Categorical(policy)\n",
    "        \n",
    "        #action = old_m.sample()\n",
    "        action = old_m.sample()\n",
    "        print(action)\n",
    "        \n",
    "        #pag 클릭. rest action은 -1 -1 이므로 실행하지 않음\n",
    "        if(action!=10):\n",
    "            # 화면 클릭\n",
    "            pag.click(action_list[action][0], action_list[action][1])\n",
    "        \n",
    "        #actions.append(action)\n",
    "        actions.append(action)\n",
    "        \n",
    "        #old_log_policy = old_m.log_prob(action)\n",
    "        old_log_policy = old_m.log_prob(action)\n",
    "        \n",
    "        #old_log_policies.append(old_log_policy)\n",
    "        old_log_policies.append(old_log_policy)\n",
    "        \n",
    "        #단위시간당 reward 감소\n",
    "        reward -= 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #state 불러오기\n",
    "        img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "        state = env.return_state(img)\n",
    "        \n",
    "       \n",
    "        \n",
    "        curr_states = curr_states.numpy()\n",
    "        #print(curr_states)\n",
    "        #train.py state = torch.from_numpy(np.concatenate(state, 0))\n",
    "        test1 = np.concatenate(curr_states, 0)\n",
    "        state = torch.from_numpy(np.concatenate(curr_states, 0))\n",
    "        \n",
    "        #train.py reward.append, dones.append\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        \n",
    "        #print action and reward\n",
    "        print('action =', action_list_name[a], '    index = ', action_list[a][0], action_list[a][1])\n",
    "        print(\"reward = \", reward)\n",
    "        \n",
    "        #curr_states = state\n",
    "        img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "        curr_states = env.return_state(img)\n",
    "        \n",
    "        if(done == 1):\n",
    "            break\n",
    "    \n",
    "    #print(\"after the corner1\")\n",
    "    \n",
    "    _, next_value, = model(curr_states)\n",
    "    next_value = next_value.squeeze()\n",
    "    old_log_policies = torch.cat(old_log_policies).detach()\n",
    "    actions = torch.cat(actions)\n",
    "    #print(f\"values = {values}\")\n",
    "    #values = torch.cat(values).detach()\n",
    "    values = torch.tensor(values).detach()\n",
    "    states = torch.cat(states)\n",
    "    gae = 0\n",
    "    R = []\n",
    "    #print(\"after the corner2\")\n",
    "    \n",
    "    for value, reward, done in list(zip(values, rewards, dones))[::-1]:\n",
    "        #opt.gamma 0.9, opt.tau = 1.0\n",
    "        gamma = 0.9\n",
    "        tau = 1.0\n",
    "        gae = gae * gamma * tau\n",
    "        gae = gae + reward + gamma * next_value.detach() * (1 - done) - value.detach()\n",
    "        next_value = value\n",
    "        R.append(gae + value)\n",
    "    R = R[::-1]\n",
    "    #R = torch.cat(R).detach()\n",
    "    R = torch.tensor(R).detach()\n",
    "    advantages = R - values\n",
    "    \n",
    "    #print(\"after the corner3\")\n",
    "    #opt.num_epochs = 10\n",
    "    num_epochs = 3\n",
    "    for i in range(num_epochs):\n",
    "        #opt.num_local_steps = 512 but 우리는 몇 step을 했는지를 카운트하여 대체\n",
    "        #opt.num_processes은 8\n",
    "        num_processes = 8\n",
    "        \n",
    "        #opt.batch_size = 16\n",
    "        batch_size = 2\n",
    "        \n",
    "        #indice = torch.randperm(num_local_steps * num_processes)\n",
    "        indice = torch.randperm(num_local_steps)\n",
    "        \n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            #이것은 어떻게 가능한가?\n",
    "#             batch_indices = indice[\n",
    "#                             int(j * (num_local_steps * num_processes / batch_size)): int((j + 1) * (\n",
    "#                                     num_local_steps * num_processes / batch_size))]\n",
    "\n",
    "            #indice copy concept\n",
    "            #states 기준으로 편집\n",
    "            indice2 = sample(range(num_local_steps), int(num_local_steps/batch_size))\n",
    "            #이렇게 짜면 후반 부담이 늘 수 있음. 개선 방안 필요.\n",
    "        \n",
    "            batch_indices = torch.tensor(indice2)\n",
    "    \n",
    "#             print(f\"start page = {int(j * (num_local_steps * num_processes / batch_size))},\\\n",
    "#                             end page = {int((j + 1) * (num_local_steps * num_processes / batch_size))}\")\n",
    "#             print(f\"0 size = {indice.size()}\")\n",
    "#             print(f\"1 size = {batch_indices}\")\n",
    "#             print(f\"2 size = {len(states)}\")\n",
    "#             print(f\"3 size = {states.shape}\")\n",
    "#             print(f\"4 size = {states[batch_indices]}\")\n",
    "            \n",
    "            logits, value = model(states[batch_indices])\n",
    "            \n",
    "            new_policy = F.softmax(logits, dim=1)\n",
    "            new_m = Categorical(new_policy)\n",
    "            new_log_policy = new_m.log_prob(actions[batch_indices])\n",
    "            ratio = torch.exp(new_log_policy - old_log_policies[batch_indices])\n",
    "            \n",
    "            #opt.epsilon = 0.2\n",
    "            epsilon = 0.2\n",
    "            \n",
    "            actor_loss = -torch.mean(torch.min(ratio * advantages[batch_indices],\n",
    "                                               torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon) *\n",
    "                                               advantages[\n",
    "                                                   batch_indices]))\n",
    "            # critic_loss = torch.mean((R[batch_indices] - value) ** 2) / 2\n",
    "            critic_loss = F.smooth_l1_loss(R[batch_indices], value.squeeze())\n",
    "            entropy_loss = torch.mean(new_m.entropy())\n",
    "            \n",
    "            #opt.beta = 0.01\n",
    "            beta = 0.01\n",
    "            \n",
    "            total_loss = actor_loss + critic_loss - beta * entropy_loss\n",
    "            writer.add_scalar('critic_loss/train', critic_loss, curr_episode)\n",
    "            writer.add_scalar('entropy_loss/train', entropy_loss, curr_episode)\n",
    "            writer.add_scalar('total_loss/train', total_loss, curr_episode)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            train_time += 1\n",
    "            print(f\"how many time to train? = {train_time}\")\n",
    "            #opt.save_interval = 50\n",
    "            save_interval = 50\n",
    "        print(\"Episode: {}. Total loss: {}, save_interval = {}\".format(curr_episode, total_loss, curr_episode % save_interval))\n",
    "        env.retryGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da13aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0b47f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2720, 550]\n"
     ]
    }
   ],
   "source": [
    "print(action_list[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
