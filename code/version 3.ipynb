{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970e56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pyautogui as pag\n",
    "import PIL\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1d1fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 20:33:25.323507: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "writer = SummaryWriter()\n",
    "train_time = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94ef1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN 모델 정의\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,num_actions):\n",
    "        print(f\"0 / {time.time()}\")\n",
    "        super(DQN, self).__init__()\n",
    "        print(f\"1 / {time.time()}\")\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        print(f\"2 / {time.time()}\")\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 64, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        print(f\"3 / {time.time()}\")\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 256, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        print(f\"4 / {time.time()}\")\n",
    "        self.actor_linear = nn.Sequential(nn.Linear(1851392, 256),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(256,num_actions))\n",
    "        print(f\"5 / {time.time()}\")\n",
    "\n",
    "    def forward(self, x): #각 action에 대한 가치를 softmax를 거쳐서 확률로 출력\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.actor_linear(x.reshape(x.size(0), -1))\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0, 10)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ffc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = [[2720, 550], [2860, 550], [3000, 550], [2720 ,660], [2860, 660], [3000, 660], [2765, 900], [2855, 900], [2950, 900], [3055, 900], [-1, -1]]\n",
    "#action list 정의. action list는 6개의 필드 위치와 4개의 카드 위치 그리고 한개의 아무것도 안하는 리워드를 주었다.\n",
    "action_list_name = {0:'left top', 1:'center top', 2:'right top', 3:'right bottom', 4:'center bottom', 5:'center right', 6:'card 1', 7:'card 2', 8:'card 3', 9:'card 4', 10:'rest action'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b130d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV():\n",
    "    def __init__(self):\n",
    "        #screenshot의 위치 지정, 클래스 생성할때 가져오기\n",
    "        \n",
    "        #winflg와 lose flag 존재해야 함. 0으로 하는 건 grayscale\n",
    "        self.winFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/weWin.png', 0)\n",
    "        self.loseFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemyWin.png', 0)\n",
    "        #nocard flag\n",
    "        self.nocardFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/nocard.png')\n",
    "        #noElixir flag\n",
    "        self.noelixirFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/noElixir.png')\n",
    "    \n",
    "    def return_state(self, img):\n",
    "        #스크린 샷을 인자로 받아와서 모델에 넣을 수 있도록 tensor로 변환\n",
    "        tf = transforms.ToTensor()\n",
    "        img_t = tf(img)\n",
    "        img_t = img_t.unsqueeze(0)\n",
    "        #img_t = img_t.permute(1, 0, 2, 3)\n",
    "        \n",
    "        \n",
    "        return img_t\n",
    "    \n",
    "    def check_win(self, img):\n",
    "        #게임이 이겼는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        #img = np.array(img)\n",
    "        checkFlag1 = np.array(img.crop((225,335,280,365)))\n",
    "        checkFlag1 = cv2.cvtColor(checkFlag1, cv2.COLOR_BGR2GRAY)\n",
    "        win_check=cv2.matchTemplate(checkFlag1,self.winFlag,cv2.TM_CCOEFF_NORMED)\n",
    "        if win_check > 0.8:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def check_lose(self, img):\n",
    "        #게임이 졌는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        #img = np.array(img)\n",
    "        checkFlag2 = np.array(img.crop((225,85,280,115)))\n",
    "        checkFlag2 = cv2.cvtColor(checkFlag2, cv2.COLOR_BGR2GRAY)\n",
    "        lose_check = cv2.matchTemplate(checkFlag2,self.loseFlag,cv2.TM_CCOEFF_NORMED)\n",
    "        if lose_check > 0.8:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def check_card(self, img):\n",
    "        #카드를 선택하지 않았는지 확인, screenshot을 가져와서 init에 지정된 nocard 이미지와 비교하여 reward 부여\n",
    "        nocard = cv2.cvtColor(self.nocardFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(nocard,img,cv2.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        if(np.max(ratio) > 0.90):\n",
    "            #print(np.max(ratio))\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def check_elixir(self, img):\n",
    "        noElixir = cv2.cvtColor(self.noelixirFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(noElixir,img,cv2.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        if(np.max(ratio) > 0.90):\n",
    "            #print(np.max(ratio))\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def checkET1(self, img):\n",
    "        score1 = 0\n",
    "        checkFlag1 = np.array(img.crop((99,135,149,136)))\n",
    "        \n",
    "        for i in range(50):\n",
    "            if(checkFlag1[0][i][0]<=96):\n",
    "                score1 += 1\n",
    "                \n",
    "        score1 = score1 * 2\n",
    "        return score1\n",
    "        \n",
    "    def checkET2(self, img):\n",
    "        score2 = 0\n",
    "        checkFlag2 = np.array(play_screen.crop((356,135,406,136)))\n",
    "        \n",
    "        for i in range(50):\n",
    "            if(checkFlag2[0][i][0]<=96):\n",
    "                score2 += 1\n",
    "                \n",
    "        score2 = score2 * 2\n",
    "        return score2\n",
    "    \n",
    "    def checkGameStart(self, img):\n",
    "        startMessage = cv2.cvtColor(self.startGameFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(startMessage,img,cv2.TM_CCOEFF_NORMED)\n",
    "        if(np.max(ratio) > 0.90):\n",
    "            #print(np.max(ratio))\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    #def checkET3\n",
    "    \n",
    "    #def checkOT1\n",
    "    #def checkOT1\n",
    "    #def checkOT1\n",
    "    \n",
    "    #우리 타워와 상대 타워의 hp를 확인하여 reward 부여\n",
    "    \n",
    "    \n",
    "    def retryGame(self):\n",
    "        pag.click((2860, 875))\n",
    "        time.sleep(2)\n",
    "        pag.click((3070, 185))\n",
    "        time.sleep(1)\n",
    "        pag.click((2920, 385))\n",
    "        time.sleep(0.5)\n",
    "        pag.click((2950, 615))\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad6722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque()\n",
    "        self.batch_size = 5\n",
    "        self.size_limit = 50000\n",
    "        \n",
    "    def put(self, data):\n",
    "        self.buffer.append(data)\n",
    "        if len(self.buffer) > self.size_limit:\n",
    "            self.buffer.popleft()\n",
    "            \n",
    "    def sample(self, n):\n",
    "        print(f\"self.buffer = {len(self.buffer)}\")\n",
    "        return random.sample(self.buffer, n)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128c3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, gamma, optimizer, batch_size):\n",
    "    global train_time\n",
    "    for i in range(3):\n",
    "        #print(f\"batch_size = {batch_size}, memory.size = {memory.size()}\")\n",
    "        batch = memory.sample(batch_size)\n",
    "        #print(f\"size = {len(batch[0])}\")\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "            \n",
    "        s_lst = torch.stack(s_lst)\n",
    "        s_prime_lst = torch.stack(s_prime_lst)\n",
    "        #print(f\"about s = {type(s_lst)}\")\n",
    "        s, a, r, s_prime, done_mask = s_lst, torch.tensor(a_lst), \\\n",
    "                                    torch.tensor(r_lst), s_prime_lst,\\\n",
    "                                    torch.tensor(done_mask_lst)\n",
    "        \n",
    "        #print(f\"about S = {type(s)}\")\n",
    "        s = s.squeeze()\n",
    "        s_prime = s_prime.squeeze()\n",
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1, a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(target, q_a)\n",
    "        print(f\"loss = {loss}\")\n",
    "        \n",
    "        \n",
    "        train_time += 1\n",
    "        writer.add_scalar(\"Loss/train\", loss, train_time)\n",
    "        writer.flush()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        print(\"we got the end1\")\n",
    "        loss.backward()\n",
    "        print(\"we got the end2\")\n",
    "        optimizer.step()\n",
    "        print(\"we got the end3\")\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5bbc9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1688297621.6678555\n",
      "1 / 1688297621.6679196\n",
      "2 / 1688297621.7024746\n",
      "3 / 1688297621.7027388\n",
      "4 / 1688297621.703194\n",
      "5 / 1688297623.080815\n",
      "0 / 1688297623.0809405\n",
      "1 / 1688297623.080954\n",
      "2 / 1688297623.0813975\n",
      "3 / 1688297623.081546\n",
      "4 / 1688297623.0819905\n",
      "5 / 1688297624.4755259\n"
     ]
    }
   ],
   "source": [
    "env = ENV()\n",
    "q = DQN(len(action_list))\n",
    "q_target = DQN(len(action_list))\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "avg_t = 0\n",
    "gamma = 0.98\n",
    "batch_size = 20\n",
    "optimizer = optim.Adam(q.parameters(), lr = 0.0005)\n",
    "\n",
    "middleBuffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4d2351",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action = center top     index =  2860 550     reward =  1999\n",
      "action = card 4     index =  3055 900     reward =  1998\n",
      "action = card 1     index =  2765 900     reward =  1997\n",
      "action = left top     index =  2720 550     reward =  1996\n",
      "action = left top     index =  2720 550     reward =  1995\n",
      "action = left top     index =  2720 550     reward =  1994\n",
      "action = center top     index =  2860 550     reward =  1993\n",
      "action = center right     index =  3000 660     reward =  1992\n",
      "action = left top     index =  2720 550     reward =  1991\n",
      "action = card 1     index =  2765 900     reward =  1990\n",
      "action = left top     index =  2720 550     reward =  1989\n",
      "action = right bottom     index =  2720 660     reward =  1988\n",
      "action = card 4     index =  3055 900     reward =  1987\n",
      "action = card 1     index =  2765 900     reward =  1986\n",
      "action = center right     index =  3000 660     reward =  1985\n",
      "action = left top     index =  2720 550     reward =  1984\n",
      "action = rest action     index =  -1 -1     reward =  1983\n",
      "action = left top     index =  2720 550     reward =  1982\n",
      "action = left top     index =  2720 550     reward =  1981\n",
      "action = left top     index =  2720 550     reward =  1980\n",
      "action = center bottom     index =  2860 660     reward =  1979\n",
      "action = card 2     index =  2855 900     reward =  1978\n",
      "action = right bottom     index =  2720 660     reward =  1977\n",
      "action = left top     index =  2720 550     reward =  1976\n",
      "action = center top     index =  2860 550     reward =  1975\n",
      "action = left top     index =  2720 550     reward =  1974\n",
      "action = rest action     index =  -1 -1     reward =  1973\n",
      "action = center top     index =  2860 550     reward =  1972\n",
      "action = left top     index =  2720 550     reward =  1971\n",
      "action = left top     index =  2720 550     reward =  1970\n",
      "action = left top     index =  2720 550     reward =  1969\n",
      "action = center top     index =  2860 550     reward =  1968\n",
      "action = left top     index =  2720 550     reward =  1967\n",
      "action = center right     index =  3000 660     reward =  1966\n",
      "action = left top     index =  2720 550     reward =  1965\n",
      "action = center bottom     index =  2860 660     reward =  1964\n",
      "action = left top     index =  2720 550     reward =  1963\n",
      "action = left top     index =  2720 550     reward =  1962\n",
      "action = card 4     index =  3055 900     reward =  1961\n",
      "action = rest action     index =  -1 -1     reward =  1960\n",
      "action = center bottom     index =  2860 660     reward =  1959\n",
      "action = card 3     index =  2950 900     reward =  1958\n",
      "action = center top     index =  2860 550     reward =  1957\n",
      "action = card 2     index =  2855 900     reward =  1956\n",
      "action = card 4     index =  3055 900     reward =  1955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m img_t \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreturn_state(img)\n\u001b[1;32m     27\u001b[0m s_prime \u001b[38;5;241m=\u001b[39m img_t\n\u001b[0;32m---> 28\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#화면 클릭\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#pag.click(action_list[a][0], action_list[a][1])\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#승리 확인\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(env\u001b[38;5;241m.\u001b[39mcheck_win(img)):\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mDQN.sample_action\u001b[0;34m(self, obs, epsilon)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs, epsilon):\n\u001b[0;32m---> 27\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     coin \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom()\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coin \u001b[38;5;241m<\u001b[39m epsilon:\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_book/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_book/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_book/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_book/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epi = 0\n",
    "done = 0\n",
    "import time\n",
    "while(1):\n",
    "    #s 변수 설정\n",
    "    img = pag.screenshot(region = (2605, 100, 510, 900))\n",
    "    img_t = env.return_state(img)\n",
    "    s = img_t\n",
    "    \n",
    "    #reward 초기화\n",
    "    reward = 0\n",
    "    #enemy tower score / score1 = left, score2 = right\n",
    "    score1 = 100\n",
    "    score2 = 100\n",
    "    t1 = time.time()\n",
    "    while(1):\n",
    "        while(1):\n",
    "            #스크린샷 찍기\n",
    "            img = pag.screenshot(region = (2605, 100, 510, 900))\n",
    "\n",
    "            \n",
    "            if(env.checkGameStart(img)):\n",
    "                #print(\"game start\")\n",
    "                time.delay(3)\n",
    "                break\n",
    "        #epsilon 조절\n",
    "        epsilon = max(0.1, 0.5 - 0.01 * (train_time/200))\n",
    "        n_epi += 1\n",
    "        reward -= 1\n",
    "        \n",
    "        #스크린샷 찍기\n",
    "        img = pag.screenshot(region = (2605, 100, 510, 900))\n",
    "        \n",
    "        #모델에 넣기 위해 tensor로 변환\n",
    "        img_t = env.return_state(img)\n",
    "        s_prime = img_t\n",
    "        a = q.sample_action(img_t, epsilon)\n",
    "        \n",
    "        #화면 클릭\n",
    "        #pag.click(action_list[a][0], action_list[a][1])\n",
    "        \n",
    "        #승리 확인\n",
    "        if(env.check_win(img)):\n",
    "            print(\"win\")\n",
    "            reward += 10000\n",
    "            done = 1\n",
    "            \n",
    "\n",
    "        #패배 확인\n",
    "        elif(env.check_lose(img)):\n",
    "            print(\"lose\")\n",
    "            reward -= 10000\n",
    "            done = 1\n",
    "            \n",
    "\n",
    "        #no card확인\n",
    "        if(env.check_card(img)):\n",
    "            reward -= 100\n",
    "            print(\"no card\")\n",
    "            \n",
    "        #no elixir확인\n",
    "        if(env.check_elixir(img)):\n",
    "            reward -= 100\n",
    "            print(\"no Elixir\")\n",
    "            \n",
    "        #enemy tower reward calculate\n",
    "        score1_now = env.checkET1(img)\n",
    "        score2_now = env.checkET1(img)\n",
    "        #print(f\"score1 = {score1}, score1_now = {score1_now}\")\n",
    "        #print(f\"score2 = {score2}, score2_now = {score2_now}\")\n",
    "        if(score1_now < score1):\n",
    "            \n",
    "            reward += 10 * (score1 - score1_now)\n",
    "            score1 = score1_now\n",
    "        \n",
    "        if(score2_now < score2):\n",
    "            \n",
    "            reward += 10 * (score2 - score2_now)\n",
    "            score2 = score2_now\n",
    "        \n",
    "        \n",
    "        middleBuffer.append((s, a, reward, s_prime, done))\n",
    "        print('action =',action_list_name[a], '    index = ', action_list[a][0], action_list[a][1], '    reward = ', reward)\n",
    "        \n",
    "        s = s_prime\n",
    "        \n",
    "        if done == 1:\n",
    "            break\n",
    "            \n",
    "    #memory size 2000넘으면 학습 시작\n",
    "    if len(middleBuffer) >= 20:\n",
    "        print(time.time() - t1)\n",
    "        for i in range(len(middleBuffer)):\n",
    "            memory.put(middleBuffer[i])\n",
    "\n",
    "        print(len(memory.buffer))\n",
    "        print(\"========================\")\n",
    "        #print(middleBuffer.size())\n",
    "        middleBuffer = []\n",
    "\n",
    "\n",
    "        train(q, q_target, memory, gamma, optimizer, batch_size)\n",
    "\n",
    "        print(f\"train time = {train_time}\")\n",
    "        q_target.load_state_dict(q.state_dict())\n",
    "        print(f\"epi = {n_epi}, buffer size = {memory.size()}, epsilon = {epsilon}\")\n",
    "\n",
    "         \n",
    "\n",
    "    #reak        \n",
    "    env.retryGame()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a978f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.retryGame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
