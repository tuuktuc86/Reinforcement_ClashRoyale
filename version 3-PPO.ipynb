{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "970e56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pyautogui as pag\n",
    "import PIL\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d1d1fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "writer = SummaryWriter()\n",
    "train_time = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e94ef1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN 모델 정의\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(PPO, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.actor_linear = nn.Sequential(nn.Linear(462848, 256),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(256, num_actions))\n",
    "        self.critic_linear = nn.Sequential(nn.Linear(462848, 256),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(256, 1))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #x = F.softmax(x, dim=softmax_dim)\n",
    "        return self.actor_linear(x), self.critic_linear(x)\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0, 10)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94fc6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV():\n",
    "    def __init__(self):\n",
    "        #screenshot의 위치 지정, 클래스 생성할때 가져오기\n",
    "        \n",
    "        #winflg와 lose flag 존재해야 함. 0으로 하는 건 grayscale\n",
    "        self.winFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/weWin.png', 0)\n",
    "        self.loseFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/cr_test/enemyWin.png', 0)\n",
    "        #nocard flag\n",
    "        self.nocardFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/nocard.png')\n",
    "        #noElixir flag\n",
    "        self.noelixirFlag = cv2.imread('/home/jnu/Desktop/Reinforce/Royale/screenshot/test1/noElixir.png')\n",
    "    \n",
    "    def return_state(self, img):\n",
    "        #스크린 샷을 인자로 받아와서 모델에 넣을 수 있도록 tensor로 변환\n",
    "        tf = transforms.ToTensor()\n",
    "        img_t = tf(img)\n",
    "        img_t = img_t.unsqueeze(0)\n",
    "        #img_t = img_t.permute(1, 0, 2, 3)\n",
    "        \n",
    "        \n",
    "        return img_t\n",
    "    \n",
    "    def check_win(self, img):\n",
    "        #게임이 이겼는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        #img = np.array(img)\n",
    "        checkFlag1 = np.array(img.crop((225,335,280,365)))\n",
    "        checkFlag1 = cv2.cvtColor(checkFlag1, cv2.COLOR_BGR2GRAY)\n",
    "        win_check=cv2.matchTemplate(checkFlag1,self.winFlag,cv2.TM_CCOEFF_NORMED)\n",
    "        if win_check > 0.8:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def check_lose(self, img):\n",
    "        #게임이 졌는지 확인, screenshot을 가져와서 우리가 원하는 크기로 잘라서 확인\n",
    "        #img = np.array(img)\n",
    "        checkFlag2 = np.array(img.crop((225,85,280,115)))\n",
    "        checkFlag2 = cv2.cvtColor(checkFlag2, cv2.COLOR_BGR2GRAY)\n",
    "        lose_check = cv2.matchTemplate(checkFlag2,self.loseFlag,cv2.TM_CCOEFF_NORMED)\n",
    "        if lose_check > 0.8:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def check_card(self, img):\n",
    "        #카드를 선택하지 않았는지 확인, screenshot을 가져와서 init에 지정된 nocard 이미지와 비교하여 reward 부여\n",
    "        nocard = cv2.cvtColor(self.nocardFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(nocard,img,cv2.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        if(np.max(ratio) > 0.90):\n",
    "            #print(np.max(ratio))\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def check_elixir(self, img):\n",
    "        noElixir = cv2.cvtColor(self.noelixirFlag, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ratio = cv2.matchTemplate(noElixir,img,cv2.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        if(np.max(ratio) > 0.90):\n",
    "            #print(np.max(ratio))\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def checkET1(self, img):\n",
    "        score1 = 0\n",
    "        checkFlag1 = np.array(img.crop((99,135,149,136)))\n",
    "        \n",
    "        for i in range(50):\n",
    "            if(checkFlag1[0][i][0]<=96):\n",
    "                score1 += 1\n",
    "                \n",
    "        score1 = score1 * 2\n",
    "        return score1\n",
    "        \n",
    "    def checkET2(self, img):\n",
    "        score2 = 0\n",
    "        checkFlag2 = np.array(play_screen.crop((356,135,406,136)))\n",
    "        \n",
    "        for i in range(50):\n",
    "            if(checkFlag2[0][i][0]<=96):\n",
    "                score2 += 1\n",
    "                \n",
    "        score2 = score2 * 2\n",
    "        return score2\n",
    "        \n",
    "    #def checkET3\n",
    "    \n",
    "    #def checkOT1\n",
    "    #def checkOT1\n",
    "    #def checkOT1\n",
    "    \n",
    "    #우리 타워와 상대 타워의 hp를 확인하여 reward 부여\n",
    "    \n",
    "    \n",
    "    def retryGame(self):\n",
    "        pag.click((2860, 875))\n",
    "        time.sleep(2)\n",
    "        pag.click((3070, 185))\n",
    "        time.sleep(1)\n",
    "        pag.click((2920, 385))\n",
    "        time.sleep(0.5)\n",
    "        pag.click((2950, 615))\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49eae7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(11)\n",
    "env = ENV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a76bad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pag.screenshot(region=(2605, 100, 510, 900))\n",
    "img_t = env.return_state(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4d9f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, value = model.forward(img_t)\n",
    "policy = F.softmax(logits, dim=1)\n",
    "old_m = Categorical(policy)\n",
    "action = old_m.sample()\n",
    "old_log_policy = old_m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df7a3358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0974,  0.0743, -0.1666,  0.2268,  0.1261,  0.0700, -0.0901, -0.0705,\n",
      "         -0.0975, -0.3052, -0.2842]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0124]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0861, 0.1022, 0.0804, 0.1191, 0.1077, 0.1018, 0.0867, 0.0885, 0.0861,\n",
      "         0.0700, 0.0714]], grad_fn=<SoftmaxBackward0>)\n",
      "Categorical(probs: torch.Size([1, 11]), logits: torch.Size([1, 11]))\n",
      "tensor([6])\n",
      "tensor([-2.4448], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n",
    "print(value)\n",
    "print(policy)\n",
    "print(old_m)\n",
    "print(action)\n",
    "print(old_log_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ffc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = [[2720, 550], [2860, 550], [3000, 550], [2720 ,660], [2860, 660], [3000, 660], [2765, 900], [2855, 900], [2950, 900], [3055, 900], [-1, -1]]\n",
    "#action list 정의. action list는 6개의 필드 위치와 4개의 카드 위치 그리고 한개의 아무것도 안하는 리워드를 주었다.\n",
    "action_list_name = {0:'left top', 1:'center top', 2:'right top', 3:'right bottom', 4:'center bottom', 5:'center right', 6:'card 1', 7:'card 2', 8:'card 3', 9:'card 4', 10:'rest action'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b130d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad6722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque()\n",
    "        self.batch_size = 5\n",
    "        self.size_limit = 50000\n",
    "        \n",
    "    def put(self, data):\n",
    "        self.buffer.append(data)\n",
    "        if len(self.buffer) > self.size_limit:\n",
    "            self.buffer.popleft()\n",
    "            \n",
    "    def sample(self, n):\n",
    "        print(f\"self.buffer = {len(self.buffer)}\")\n",
    "        return random.sample(self.buffer, n)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128c3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, gamma, optimizer, batch_size):\n",
    "    global train_time\n",
    "    for i in range(3):\n",
    "        #print(f\"batch_size = {batch_size}, memory.size = {memory.size()}\")\n",
    "        batch = memory.sample(batch_size)\n",
    "        #print(f\"size = {len(batch[0])}\")\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "            \n",
    "        s_lst = torch.stack(s_lst)\n",
    "        s_prime_lst = torch.stack(s_prime_lst)\n",
    "        #print(f\"about s = {type(s_lst)}\")\n",
    "        s, a, r, s_prime, done_mask = s_lst, torch.tensor(a_lst), \\\n",
    "                                    torch.tensor(r_lst), s_prime_lst,\\\n",
    "                                    torch.tensor(done_mask_lst)\n",
    "        \n",
    "        #print(f\"about S = {type(s)}\")\n",
    "        s = s.squeeze()\n",
    "        s_prime = s_prime.squeeze()\n",
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1, a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(target, q_a)\n",
    "        print(f\"loss = {loss}\")\n",
    "        \n",
    "        \n",
    "        train_time += 1\n",
    "        writer.add_scalar(\"Loss/train\", loss, train_time)\n",
    "        writer.flush()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        print(\"we got the end1\")\n",
    "        loss.backward()\n",
    "        print(\"we got the end2\")\n",
    "        optimizer.step()\n",
    "        print(\"we got the end3\")\n",
    "        \n",
    "        \n",
    "        if train_time >= 5:\n",
    "            \n",
    "            writer.close()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5bbc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ENV()\n",
    "q = DQN(len(action_list))\n",
    "q_target = DQN(len(action_list))\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "avg_t = 0\n",
    "gamma = 0.98\n",
    "batch_size = 5\n",
    "optimizer = optim.SGD(q.parameters(), lr = 0.0005)\n",
    "\n",
    "middleBuffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c4d2351",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action = card 3     index =  2950 900     reward =  1999\n",
      "action = card 3     index =  2950 900     reward =  1998\n",
      "action = card 2     index =  2855 900     reward =  1997\n",
      "action = card 3     index =  2950 900     reward =  1996\n",
      "action = card 4     index =  3055 900     reward =  1995\n",
      "action = card 2     index =  2855 900     reward =  1994\n",
      "action = card 3     index =  2950 900     reward =  1993\n",
      "action = card 3     index =  2950 900     reward =  1992\n",
      "action = center right     index =  3000 660     reward =  1991\n",
      "action = card 3     index =  2950 900     reward =  1990\n",
      "action = card 3     index =  2950 900     reward =  1989\n",
      "action = card 2     index =  2855 900     reward =  1988\n",
      "action = card 3     index =  2950 900     reward =  1987\n",
      "action = card 3     index =  2950 900     reward =  1986\n",
      "action = center top     index =  2860 550     reward =  1985\n",
      "action = card 3     index =  2950 900     reward =  1984\n",
      "action = card 2     index =  2855 900     reward =  1983\n",
      "action = card 3     index =  2950 900     reward =  1982\n",
      "action = card 3     index =  2950 900     reward =  1981\n",
      "action = card 3     index =  2950 900     reward =  1980\n",
      "20\n",
      "========================\n",
      "self.buffer = 20\n",
      "loss = 1992.558349609375\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 20\n",
      "loss = 1958.589111328125\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 20\n",
      "loss = 1935.2318115234375\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "train time = 3\n",
      "epi = 20, buffer size = 20, epsilon = 0.49905\n",
      "action = center right     index =  3000 660     reward =  1979\n",
      "action = card 3     index =  2950 900     reward =  1978\n",
      "action = card 3     index =  2950 900     reward =  1977\n",
      "action = right bottom     index =  2720 660     reward =  1976\n",
      "action = card 3     index =  2950 900     reward =  1975\n",
      "action = center bottom     index =  2860 660     reward =  1974\n",
      "action = card 3     index =  2950 900     reward =  1973\n",
      "action = left top     index =  2720 550     reward =  1972\n",
      "action = card 3     index =  2950 900     reward =  1971\n",
      "action = card 4     index =  3055 900     reward =  1970\n",
      "action = card 3     index =  2950 900     reward =  1969\n",
      "action = center bottom     index =  2860 660     reward =  1968\n",
      "action = card 3     index =  2950 900     reward =  1967\n",
      "action = rest action     index =  -1 -1     reward =  1966\n",
      "action = center top     index =  2860 550     reward =  1965\n",
      "action = card 3     index =  2950 900     reward =  1964\n",
      "action = card 3     index =  2950 900     reward =  1963\n",
      "action = card 2     index =  2855 900     reward =  1962\n",
      "action = center top     index =  2860 550     reward =  1961\n",
      "action = card 3     index =  2950 900     reward =  1960\n",
      "40\n",
      "========================\n",
      "self.buffer = 40\n",
      "loss = 1836.42578125\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 40\n",
      "loss = 1842.2203369140625\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 40\n",
      "loss = 1683.2308349609375\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "train time = 6\n",
      "epi = 40, buffer size = 40, epsilon = 0.49805\n",
      "action = card 3     index =  2950 900     reward =  1959\n",
      "action = card 3     index =  2950 900     reward =  1958\n",
      "action = card 3     index =  2950 900     reward =  1957\n",
      "action = card 3     index =  2950 900     reward =  1956\n",
      "action = center bottom     index =  2860 660     reward =  1955\n",
      "action = card 1     index =  2765 900     reward =  1954\n",
      "action = card 3     index =  2950 900     reward =  1953\n",
      "action = card 3     index =  2950 900     reward =  1952\n",
      "action = card 3     index =  2950 900     reward =  1951\n",
      "action = card 3     index =  2950 900     reward =  1950\n",
      "action = card 3     index =  2950 900     reward =  1949\n",
      "action = card 3     index =  2950 900     reward =  1948\n",
      "action = card 3     index =  2950 900     reward =  1947\n",
      "action = card 3     index =  2950 900     reward =  1946\n",
      "action = card 3     index =  2950 900     reward =  1945\n",
      "action = card 3     index =  2950 900     reward =  1944\n",
      "action = center bottom     index =  2860 660     reward =  1943\n",
      "action = card 3     index =  2950 900     reward =  1942\n",
      "action = right top     index =  3000 550     reward =  1941\n",
      "action = card 3     index =  2950 900     reward =  1940\n",
      "60\n",
      "========================\n",
      "self.buffer = 60\n",
      "loss = 1716.6119384765625\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 60\n",
      "loss = 1378.8765869140625\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 60\n",
      "loss = 919.0221557617188\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "train time = 9\n",
      "epi = 60, buffer size = 60, epsilon = 0.49705\n",
      "action = rest action     index =  -1 -1     reward =  1939\n",
      "action = card 3     index =  2950 900     reward =  1938\n",
      "action = card 3     index =  2950 900     reward =  1937\n",
      "action = card 3     index =  2950 900     reward =  1936\n",
      "action = card 3     index =  2950 900     reward =  1935\n",
      "action = card 3     index =  2950 900     reward =  1934\n",
      "action = card 3     index =  2950 900     reward =  1933\n",
      "action = center right     index =  3000 660     reward =  1932\n",
      "action = rest action     index =  -1 -1     reward =  1931\n",
      "action = center right     index =  3000 660     reward =  1930\n",
      "action = center top     index =  2860 550     reward =  1929\n",
      "action = center bottom     index =  2860 660     reward =  1928\n",
      "action = card 2     index =  2855 900     reward =  1927\n",
      "action = card 2     index =  2855 900     reward =  1926\n",
      "action = card 3     index =  2950 900     reward =  1925\n",
      "action = rest action     index =  -1 -1     reward =  1924\n",
      "action = card 4     index =  3055 900     reward =  1923\n",
      "action = card 3     index =  2950 900     reward =  1922\n",
      "action = card 3     index =  2950 900     reward =  1921\n",
      "action = center right     index =  3000 660     reward =  1920\n",
      "80\n",
      "========================\n",
      "self.buffer = 80\n",
      "loss = 1109.1793212890625\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 80\n",
      "loss = 834.759765625\n",
      "we got the end1\n",
      "we got the end2\n",
      "we got the end3\n",
      "self.buffer = 80\n",
      "loss = 1042.34423828125\n",
      "we got the end1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m#print(middleBuffer.size())\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     middleBuffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain time = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_epi\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\u001b[38;5;129;01mand\u001b[39;00m n_epi\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[15], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(q, q_target, memory, gamma, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwe got the end1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwe got the end2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_book/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_book/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epi = 0\n",
    "done = 0\n",
    "while(1):\n",
    "    #s 변수 설정\n",
    "    img = pag.screenshot(region = (2605, 100, 510, 900))\n",
    "    img_t = env.return_state(img)\n",
    "    s = img_t\n",
    "    \n",
    "    #reward 초기화\n",
    "    reward = 0\n",
    "    #enemy tower score / score1 = left, score2 = right\n",
    "    score1 = 100\n",
    "    score2 = 100\n",
    "    \n",
    "    while(1):\n",
    "        #epsilon 조절\n",
    "        epsilon = max(0.1, 0.5 - 0.01 * (n_epi/200))\n",
    "        n_epi += 1\n",
    "        reward -= 1\n",
    "        \n",
    "        #스크린샷 찍기\n",
    "        img = pag.screenshot(region = (2605, 100, 510, 900))\n",
    "        \n",
    "        #모델에 넣기 위해 tensor로 변환\n",
    "        img_t = env.return_state(img)\n",
    "        s_prime = img_t\n",
    "        a = q.sample_action(img_t, epsilon)\n",
    "        \n",
    "        #화면 클릭\n",
    "        #pag.click(action_list[a][0], action_list[a][1])\n",
    "        \n",
    "        #승리 확인\n",
    "        if(env.check_win(img)):\n",
    "            print(\"win\")\n",
    "            reward += 10000\n",
    "            done = 1\n",
    "            \n",
    "\n",
    "        #패배 확인\n",
    "        elif(env.check_lose(img)):\n",
    "            print(\"lose\")\n",
    "            reward -= 10000\n",
    "            done = 1\n",
    "            \n",
    "\n",
    "        #no card확인\n",
    "        if(env.check_card(img)):\n",
    "            reward -= 100\n",
    "            print(\"no card\")\n",
    "            \n",
    "        #no elixir확인\n",
    "        if(env.check_elixir(img)):\n",
    "            reward -= 100\n",
    "            print(\"no Elixir\")\n",
    "            \n",
    "        #enemy tower reward calculate\n",
    "        score1_now = env.checkET1(img)\n",
    "        score2_now = env.checkET1(img)\n",
    "        #print(f\"score1 = {score1}, score1_now = {score1_now}\")\n",
    "        #print(f\"score2 = {score2}, score2_now = {score2_now}\")\n",
    "        if(score1_now < score1):\n",
    "            \n",
    "            reward += 10 * (score1 - score1_now)\n",
    "            score1 = score1_now\n",
    "        \n",
    "        if(score2_now < score2):\n",
    "            \n",
    "            reward += 10 * (score2 - score2_now)\n",
    "            score2 = score2_now\n",
    "        \n",
    "        \n",
    "        middleBuffer.append((s, a, reward, s_prime, done))\n",
    "        print('action =',action_list_name[a], '    index = ', action_list[a][0], action_list[a][1], '    reward = ', reward)\n",
    "        \n",
    "        s = s_prime\n",
    "        \n",
    "        if done == 1:\n",
    "            break\n",
    "            \n",
    "        #memory size 2000넘으면 학습 시작\n",
    "        if len(middleBuffer) >= 20:\n",
    "            for i in range(len(middleBuffer)):\n",
    "                memory.put(middleBuffer[i])\n",
    "\n",
    "            print(len(memory.buffer))\n",
    "            print(\"========================\")\n",
    "            #print(middleBuffer.size())\n",
    "            middleBuffer = []\n",
    "\n",
    "\n",
    "            train(q, q_target, memory, gamma, optimizer, batch_size)\n",
    "            \n",
    "            print(f\"train time = {train_time}\")\n",
    "            \n",
    "        \n",
    "\n",
    "        if n_epi%20 == 0and n_epi!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(f\"epi = {n_epi}, buffer size = {memory.size()}, epsilon = {epsilon}\") \n",
    "\n",
    "    #reak        \n",
    "    env.retryGame()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a978f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
