{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9b0de1-7e8f-438e-b825-8f21ba28bcae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnu/anaconda3/envs/jupyter/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d972e3be-ced1-436e-b01e-933b67f2b508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DQN 모델 정의\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 64, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 256, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(512),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.actor_linear = nn.Sequential(nn.Linear(933888, 256),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(256,num_actions))\n",
    "\n",
    "    def forward(self, x): #각 action에 대한 가치출력\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.actor_linear(x.reshape(x.size(0), -1))\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self,obs,epsilon):\n",
    "        out=self.forward(obs)\n",
    "        coin=random.random()\n",
    "        if coin<epsilon:\n",
    "            return random.randint(0,509)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b6e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self,num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 64, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 256, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(512),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.actor_linear = nn.Sequential(nn.Linear(933888, 256),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.Linear(256,num_actions))\n",
    "def __init__(self,num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 64, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 256, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(512),\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        self.actor_linear = nn.Sequential(nn.Linear(933888, 13))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ddc688-a4af-45e4-95e2-2543461e8dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            state, action, reward, s_prime, done_mask = transition\n",
    "            s_lst.append(state)\n",
    "            a_lst.append([action])\n",
    "            r_lst.append([reward])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26e0440-11a3-4252-9553-d21ae1d969e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_list=[]\n",
    "y_list=[]\n",
    "for i in range(14):\n",
    "    y_list.append(508+(19*i))\n",
    "    y_list.append(202+(19*i))\n",
    "    \n",
    "for i in range(18):\n",
    "    x_list.append(791+(24*i))\n",
    "    action_list=[(x,y) for x in x_list for y in y_list]\n",
    "    action_list.append((900,900)) #1번 카드(252)\n",
    "    action_list.append((1000,900)) #2번 카드(253)\n",
    "    action_list.append((1100,900)) #3번 카드(254)\n",
    "    action_list.append((1200,900)) #4번 카드(255)\n",
    "    action_list.append((858,478)) #왼쪽 다리\n",
    "    action_list.append((1125,478)) #오른쪽 다리\n",
    "    num_actions=len(action_list)\n",
    "\n",
    "action_set={}\n",
    "for i in range(num_actions):\n",
    "    action_set[i]=action_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896648d0-1721-4b02-a8b6-3b1d4286362b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size    = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ae8b4e-397a-4ba2-8c87-3ee58400adae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#게임 환경 클래스(state, reward, done 이 들어가야함)\n",
    "class ENV:\n",
    "    def __init__(self):\n",
    "        self.done=0\n",
    "        self.reward=0\n",
    "        \n",
    "    def get_state(self):\n",
    "        screenshot=pyautogui.screenshot(region=(741,100,1250-741,999-100))\n",
    "        screenshot.save('screenshot.png')\n",
    "        state=np.array(screenshot)\n",
    "        state=np.float32(state)/255.0\n",
    "        state=torch.from_numpy(state).unsqueeze(0).float()\n",
    "        state=state.permute(0,3,1,2)\n",
    "        return state # state는 tensor이고 (1,3,509,899)\n",
    "\n",
    "    def get_reward(self):\n",
    "        screenshot=cv2.imread('screenshot.png')\n",
    "        lose=cv2.imread('lose.png')\n",
    "        win=cv2.imread('win.png')\n",
    "        score_lose=cv2.matchTemplate(screenshot,lose,cv2.TM_CCOEFF_NORMED)\n",
    "        score_win=cv2.matchTemplate(screenshot,win,cv2.TM_CCOEFF_NORMED)\n",
    "        reward=0\n",
    "        done=0 #게임 한 번\n",
    "        if score_lose[0][0]>=0.9:\n",
    "            reward-=3000000\n",
    "            done=1\n",
    "        \n",
    "        #win 사진\n",
    "        elif score_win[0][0]>=0.9:\n",
    "            reward+=3000000\n",
    "            done=1\n",
    "            \n",
    "        return reward, done\n",
    "    \n",
    "    def get_reward2(self):\n",
    "        enemy_l=pyautogui.screenshot(region=(825,223,900-825,243-223))\n",
    "        enemy_r=pyautogui.screenshot(region=(1090,223,1165-1090,243-223))\n",
    "        enemy_m=pyautogui.screenshot(region=(942,114,1052-942,145-114))\n",
    "        my_l=pyautogui.screenshot(region=(825,648,900-825,668-648))\n",
    "        my_r=pyautogui.screenshot(region=(1090,648,1165-1090,668-648))\n",
    "        my_m=pyautogui.screenshot(region=(942,767,1052-942,798-767))\n",
    "\n",
    "        return my_l,my_r,my_m,enemy_l,enemy_r,enemy_m\n",
    "    \n",
    "    #메뉴칸 (1223,189) #훈련캠프 (1068,384) #확인 (1082,609) #끝나고 확인버튼 (990,862)\n",
    "    def replay(self):\n",
    "        time.sleep(1)\n",
    "        pyautogui.moveTo(990,862)\n",
    "        pyautogui.click(button='left')\n",
    "        time.sleep(1)\n",
    "        pyautogui.moveTo(1223,189)\n",
    "        pyautogui.click(button='left')\n",
    "        time.sleep(1)\n",
    "        pyautogui.moveTo(1068,384)\n",
    "        pyautogui.click(button='left')\n",
    "        time.sleep(1)\n",
    "        pyautogui.moveTo(1082,609)\n",
    "        pyautogui.click(button='left')\n",
    "        \n",
    "    \n",
    "    #def select_action(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d556a7d7-9965-4cda-b988-eddc3b7c8d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1,a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f0cecc-9a2c-4037-9925-4263442c30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(image1,image2):\n",
    "    score=cv2.matchTemplate(image1,image2,cv2.TM_CCOEFF_NORMED)\n",
    "    return score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7ac6a6-58c6-476f-9ff4-b44766d74eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@9.412] global loadsave.cpp:244 findDecoder imread_('enenmy_l.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@9.413] global loadsave.cpp:244 findDecoder imread_('enenmy_r.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@9.413] global loadsave.cpp:244 findDecoder imread_('enenmy_m.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@23.501] global loadsave.cpp:244 findDecoder imread_('enenmy_l.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@23.502] global loadsave.cpp:244 findDecoder imread_('enenmy_r.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@23.502] global loadsave.cpp:244 findDecoder imread_('enenmy_m.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :20, score : -150015.0, n_buffer : 2, eps : 7.9%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = ENV()\n",
    "    q = DQN(num_actions)\n",
    "    q_target = DQN(num_actions)\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "    for n_epi in range(40):\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "        state = env.get_state()\n",
    "        reward,done=env.get_reward()[0],env.get_reward()[1]\n",
    "\n",
    "        while done==0:\n",
    "            '''\n",
    "            my_l,my_r,my_m,enemy_l,enemy_r,enemy_m=env.get_reward2()[0],env.get_reward2()[1],env.get_reward2()[2],env.get_reward2()[3],env.get_reward2()[4],env.get_reward2()[5]\n",
    "            my_l.save('my_l.png')\n",
    "            my_l=cv2.imread('my_l.png')\n",
    "            my_r.save('my_r.png')\n",
    "            my_r=cv2.imread('my_r.png')\n",
    "            my_m.save('my_m.png')\n",
    "            my_m=cv2.imread('my_m.png')\n",
    "            enemy_l.save('enemy_l.png')\n",
    "            enemy_l=cv2.imread('enenmy_l.png')\n",
    "            enemy_r.save('enemy_r.png')\n",
    "            enemy_r=cv2.imread('enenmy_r.png')\n",
    "            enemy_m.save('enemy_m.png')\n",
    "            enemy_m=cv2.imread('enenmy_m.png')\n",
    "            '''\n",
    "            action_= q.sample_action(state, epsilon)\n",
    "            action=action_set[action_]\n",
    "            \n",
    "            pyautogui.moveTo(action[0],action[1])\n",
    "            pyautogui.click(button='left')\n",
    "            '''\n",
    "            s_prime, reward, done = env.get_state(),env.get_reward()[0],env.get_reward()[1]\n",
    "            new_my_l,new_my_r,new_my_m,new_enemy_l,new_enemy_r,new_enemy_m=env.get_reward2()[0],env.get_reward2()[1],env.get_reward2()[2],env.get_reward2()[3],env.get_reward2()[4],env.get_reward2()[5]\n",
    "            new_my_l.save('new_my_l.png')\n",
    "            new_my_l=cv2.imread('new_my_l.png')\n",
    "            new_my_r.save('new_my_r.png')\n",
    "            new_my_r=cv2.imread('new_my_r.png')\n",
    "            new_my_m.save('new_my_m.png')\n",
    "            new_my_m=cv2.imread('new_my_m.png')\n",
    "            new_enemy_l.save('new_enemy_l.png')\n",
    "            new_enemy_l=cv2.imread('new_enemy_l.png')\n",
    "            new_enemy_r.save('new_enemy_r.png')\n",
    "            new_enemy_r=cv2.imread('new_enemy_r.png')\n",
    "            new_enemy_m.save('new_enemy_m.png')\n",
    "            new_enemy_m=cv2.imread('new_enemy_m.png')\n",
    "        \n",
    "            if diff(my_l,new_my_l)==1.0:\n",
    "                reward=reward\n",
    "            else:\n",
    "                reward-=100\n",
    "            \n",
    "            if diff(my_r,new_my_r)==1.0:\n",
    "                reward=reward\n",
    "            else:\n",
    "                reward-=100\n",
    "            \n",
    "            if diff(my_m,new_my_m)==1.0:\n",
    "                reward=reward\n",
    "            else:\n",
    "                reward-=100\n",
    "            '''    \n",
    "            \n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((state,action,reward/100.0,s_prime, done_mask))\n",
    "            state = s_prime\n",
    "\n",
    "            score += reward\n",
    "            \n",
    "            if done==1:\n",
    "                break\n",
    "            \n",
    "        if memory.size()>2000:\n",
    "            train(q, q_target, memory, optimizer)\n",
    "\n",
    "        \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n",
    "            score = 0.0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80c633de-3690-44e5-87c4-d17a7d8ba2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss mean : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(losses[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m steps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, step)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(steps, losses[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'loss mean : {np.mean(losses[:-1])}')\n",
    "steps = np.arange(1, step)\n",
    "plt.plot(steps, losses[:-1], 'r', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Click')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26465855-ed4b-4029-b221-a1a84be37c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
