# Reinforcement_ClashRoyale

## 개요
본 프로젝트는 클래시로얄 게임을 강화학습으로 풀어보고자 하였습니다. 이상적인 학습은 이루어지지 않았지만 강화학습을 위한 ENV 설정과 Reward 체계 구현 등에서 의미를 지니고 있습니다. 동영상으로 구현 동작을 보여주고 싶으나 프로젝트를 한지 너무 오래돼서 괜찮은 동영상이 없고 개발환경 다시 세팅하기에는 시간이 너무 오래 소요될 것 같으므로 문서를 최대한 상세하게 적겠습니다.

## 클래시 로얄이란?
클래시 로얄은 카드 수집형 실시간 타워 디펜스 게임입니다. 사용자는 카드를 선택하여 소환할 수 있으며 정해진 시간안에 상대 킹 타워를 파괴하면 승리합니다. 카드 소환에는 엘릭서라는 자원이 필요하며 카드 덱의 구성에 따라 다양한 전략이 가능합니다.

## 시작 전
ubuntu 20.04 위에서 개발되었습니다. 클래시 로얄 게임이 ubuntu에서 돌아가지 못하기 때문에 vmware로 windows를 설치하고 그 위에 blue stack을 설치하여 사용합니다.
ubuntu에서 몇가지 에뮬레이터를 설치하여 클래시 로얄을 돌려보려고 하였으나 성공하지 못하였고(개발 초기 단계에서 시간이 오래 지나 정확한 이유 잊어버림) <br>
ubuntu에서 vmware를 설치하고 그 다음 blue stack을 설치하여 게임을 플레이 하였습니다.<br>
arm 기반 architecture가 문제가 되어서 super cell에서 만든 게임을 돌리지 못했던 것으로 기억합니다.<br>
해결 방법이 있으면 알려주세요.

## ENV

설정한 ENV는 다음과 같은 함수를 포함합니다.

 -return_state(img) : 이미지를 가져와 model에 넣을 수 있는 shape로 변경한 후 이미지를 tensor 형태로 반환합니다.<br>
 -check_finish(img) : 현재 게임의 이미지를 가져와 게임이 종료되었는지 확인합니다. 게임이 종료되었으면 1, 종료되지 않았으면 0을 반환합니다.<br>
 -check_win(img) : 현재 게임의 이미지를 가져와 게임이 승리하였는지 확인합니다. 게임 승리 문구가 뜨면 1, 뜨지 않았으면 0을 반환합니다. <br>
 -check_lose(img) : 현재 게임의 이미지를 가져와 게임이 패배하였는지 확인합니다. 게임 패배 문구가 뜨면 1, 뜨지 않았으면 0을 반환합니다. <br>
 -check_card(img) : agent가 카드를 선택하지 않은 상태에서 map을 클릭하였는지 확인합니다. 카드를 선택하지 않은 상태에서 map을 뜨면 '카드를 선택하지 않았습니다' 문구가 뜹니다. 해당 문구를 발견하면 1을 반환합니다.<br>
 -check_elixir(img) : agent가 엘릭서가 부족한 상태에서 카드를 클릭하였는지 확인합니다. 엘릭서가 부족한 상태에서 카드를 클릭하면 '엘릭서가 부족합니다'라는 문구가 뜹니다. 해당 문구를 발견하면 1을 반환합니다.<br>
 -checkET1(img) : 상대 1번 타워의 hp를 확인하여 해당 값을 반환합니다. 타워 hp가 ocr로 인식이 되지 않아 hp바의 길이로 타워 hp를 확인합니다.<br>
 -checkET2(img) : 상대 2번 타워의 hp를 확인하여 해당 값을 반환합니다. 타워 hp가 ocr로 인식이 되지 않아 hp바의 길이로 타워 hp를 확인합니다.<br>
 -checkGameStart
 -checkMainPage
 -stop_game
 -enemy1
 -enemy2
 -enemy3
 -retryGame

## Action
Action은 카드를 선택할 수 있는 4가지 선택지와 map을 () * () 로 구분한 ()가지 선택지가 존재하며 총 () 개의 action을 가지고 있습니다.

## Agent
Agent는 state를 입력받아 Actiondmf 결정합니다.
모델 출력단에서 softmax를 적용하여 다음 선택지를 classification 문제로 해결할 수 있습니다.
모델은 다음과 같은 구조를 사용하였습니다.

## state
state는 게임에 영향을 주는 부분을 잘라서 사용합니다. size는 ()입니다.
사용자 설정에 따라 resize, grayscale, framestack을 적용하여 사용할 수 있습니다.

## 강화학습 알고리즘
PPO 알고리즘을 적용하여 문제를 해결하고자 하였습니다.

## tensor board

## 시연 영상

